# ABE-XAI
This repository demonstrates adversarial attacks and explanation techniques using a provied model. It includes an example of how to perform both untargeted and targeted adversarial attacks, as well as visualize adversarial samples. Additionally, it demonstrates the use of various explanation methods to interpret model predictions.

## Requirements

- Python 3.x
- PyTorch
- torchvision
- PIL
- other custom modules (`abe.algorithm`, `abe.task`, `abe.metric.visualization`, etc.)

## Setup

1. **Model Setup**  
   The code uses a pre-trained ResNet50 model for image classification. But the pipeline can be extended to support other model types such as object detection, text classification, and time series forecasting. The input image is transformed to the appropriate size and normalized using standard ImageNet values.

   ```python
   model = nn.Sequential(
       Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
       resnet50(pretrained=True)
   ).eval().cuda()
   ```

2. **Loss Function**  
   The loss function for the attack is defined using cross-entropy loss.

   ```python
   def loss_fn(batch):
       x, y = batch
       logits = model(x)
       return F.cross_entropy(logits, y)
   ```


## Adversarial Attack

1. **Define Attack Task**  
   The `AttackTask` is initialized with the loss function, model type, and whether the attack is targeted or untargeted.

   ```python
   task = AttackTask(loss_fn=loss_fn, model_type=ModelType.IMAGECLASSIFICATION, is_targeted=False)
   ```

2. **Choose Attack Algorithm**  
   Multiple attack algorithms are available: FGSM, PGD, BIM, CW, MIFGSM, etc. In this example, BIM is used.

   ```python
   attack = BIM(task)
   adversarial_x = attack([sample_x, sample_y])
   ```

3. **Targeted Attack**  
   For targeted attacks, set `is_targeted=True` and provide a target label.

   ```python
   targeted_task = AttackTask(loss_fn=loss_fn, model_type=ModelType.IMAGECLASSIFICATION, is_targeted=True)
   targeted_attack = BIM(targeted_task)
   targeted_adversarial_x = targeted_attack([sample_x, torch.zeros_like(sample_y)])
   ```

4. **Visualization**  
   Use the `plot_adversarial_sample` function to visualize the original and adversarial images.

   ```python
   plot_adversarial_sample(forward, batch=[sample_x, sample_y], adversarial_sample=adversarial_x, model_type=ModelType.IMAGECLASSIFICATION)
   ```

## Explanation

1. **Define Explanation Task**  
   The explanation task is initialized with the loss function and forward pass function.

   ```python
   def loss_fn(batch):
       x, y = batch
       logits = model(x)
       loss = -torch.diag(logits[:, y]).sum()
       return loss
   ```

2. **Explanation Algorithms**  
   Several explanation algorithms are available, such as AMPE, IG, Saliency Map, etc. In this example, AMPE is used.

   ```python
   explanation = AMPE(explanation_task)
   attribution = explanation([sample_x, sample_y])
   ```

3. **Visualization of Explanation**  
   The attribution map generated by the explanation algorithm can be visualized using the `plot_explanation_heatmap` function.

   ```python
   plot_explanation_heatmap(attribution, sample_x)
   ```

## Example Workflow

1. Load and transform an image.
2. Define the model and loss function.
3. Perform untargeted and targeted attacks.
4. Visualize adversarial samples.
5. Use an explanation method (e.g., AMPE) to generate attribution maps.
6. Visualize the attribution heatmaps.

---
