{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2,FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "from PIL import Image\n",
    "sys.path.append('..')\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "\n",
    "batch_data = pickle.load(open('sample_data/batch_data.pkl', 'rb'))\n",
    "\n",
    "sample_x, sample_y = batch_data\n",
    "# Use the Faster R-CNN model for object detection\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # Load pretrained Faster R-CNN model\n",
    "model = model.cuda()\n",
    "\n",
    "# Freeze batchnorm layers\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "        m.weight.requires_grad = False\n",
    "        m.bias.requires_grad = False\n",
    "\n",
    "target_y = copy.deepcopy(sample_y)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(sample_y)):\n",
    "    for k in sample_y[i].keys():\n",
    "        sample_y[i][k] = sample_y[i][k].cuda()\n",
    "        target_y[i][k] = target_y[i][k].cuda()\n",
    "        if k == \"labels\":\n",
    "            target_y[i][k] = torch.ones_like(sample_y[i][k])\n",
    "            \n",
    "sample_x,sample_y,target_y = sample_x[0],sample_y[0],target_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before peforimg any attack, We need to define the loss function\n",
    "# loss function takes only one argument, which is the batch of data\n",
    "# We can define the loss function as follows\n",
    "def loss_fn(model,batch):\n",
    "    # Forward pass through the Faster R-CNN model\n",
    "    # The model expects a list of tensors (each tensor is a dictionary with boxes and labels)\n",
    "    # Here, we assume sample_y is already in the correct format as a list of dictionaries\n",
    "    # To avoid effect batchnorm, we need to deepcopy the model\n",
    "\n",
    "    # Note: Faster R-CNN model in PyTorch returns a dictionary of losses\n",
    "    sample_x, sample_y = batch\n",
    "    loss_dict = model([sample_x], [sample_y])\n",
    "    \n",
    "    # Combine all losses (classification + bbox regression)\n",
    "    # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    losses = loss_dict['loss_classifier']\n",
    "    return losses\n",
    "\n",
    "from functools import partial\n",
    "loss_fn = partial(loss_fn, model) # now loss_fn takes only one argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abe.algorithm.attack import FGSM, PGD, BIM, MIFGSM, TIFGSM, DIFGSM, SINIFGSM, SSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to define the attack task\n",
    "from abe.task import AttackTask\n",
    "from abe.type import ModelType\n",
    "\n",
    "task = AttackTask(loss_fn=loss_fn, model_type=ModelType.IMAGECLASSIFICATION, is_targeted=False) # for targeted attack, set is_targeted=True\n",
    "\n",
    "# then we can define the attack algorithm\n",
    "\n",
    "attack = BIM(task,eps=32/255, alpha=2/255, steps=16) # BIM attack\n",
    "\n",
    "adversarial_x = attack([sample_x, sample_y]) # all attack algorithms take a batch of data as input\n",
    "\n",
    "targeted_task = AttackTask(loss_fn=loss_fn, model_type=ModelType.IMAGECLASSIFICATION, is_targeted=True)\n",
    "\n",
    "targeted_attack = BIM(targeted_task,eps=32/255, alpha=2/255, steps=16) # BIM attack with targeted_task\n",
    "\n",
    "targeted_adversarial_x = targeted_attack([sample_x, target_y]) # for targeted attack, the second element of the batch should be the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the adversarial example\n",
    "\n",
    "from abe.metric.visualization import plot_adversarial_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the adversarial example, we need to provide the original sample and the adversarial sample and forward function\n",
    "\n",
    "def forward(batch):\n",
    "    model.eval()\n",
    "    x, _ = batch\n",
    "    pred = model(x)\n",
    "    model.train()\n",
    "    threshold = 0.5\n",
    "    scores = pred[0]['scores']\n",
    "    boxes = pred[0]['boxes']\n",
    "    labels = pred[0]['labels']\n",
    "    keep = scores > threshold\n",
    "    scores = scores[keep]\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    return boxes, labels\n",
    "\n",
    "# we should transform the input data to the correct format N, C, H, W\n",
    "\n",
    "\n",
    "plot_adversarial_sample(forward,batch=[sample_x.unsqueeze(0),sample_y],adversarial_sample=adversarial_x.unsqueeze(0),model_type=ModelType.OBJECTDETECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_adversarial_sample(forward,batch=[sample_x.unsqueeze(0),sample_y],adversarial_sample=targeted_adversarial_x.unsqueeze(0),model_type=ModelType.OBJECTDETECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abe.algorithm.explanation import AMPE,IG, FastIG, SaliencyMap, SmoothGradient, MFABA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to define the explanation task\n",
    "from abe.task import ExplanationTask\n",
    "\n",
    "# Explanation task takes loss function and forward function as initialization arguments\n",
    "\n",
    "def loss_fn(batch):\n",
    "    # Forward pass through the Faster R-CNN model\n",
    "    # The model expects a list of tensors (each tensor is a dictionary with boxes and labels)\n",
    "    # Here, we assume sample_y is already in the correct format as a list of dictionaries\n",
    "    # To avoid effect batchnorm, we need to deepcopy the model\n",
    "    # Freeze batchnorm layers\n",
    "    # Note: Faster R-CNN model in PyTorch returns a dictionary of losses\n",
    "    sample_x, sample_y = batch\n",
    "    loss_dict = model([sample_x], [sample_y])\n",
    "    \n",
    "    # Combine all losses (classification + bbox regression)\n",
    "    # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    losses = loss_dict['loss_box_reg'] # use a specific box regression loss # not implemented in the current version, instead we use all box regression loss\n",
    "    return losses\n",
    "\n",
    "def forward(batch):\n",
    "    return model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_task = ExplanationTask(loss_fn=loss_fn, forward_fn=forward, model_type=ModelType.OBJECTDETECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can define the explanation algorithm\n",
    "\n",
    "explanation = AMPE(explanation_task)\n",
    "\n",
    "attribution = explanation([sample_x, sample_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the attribution map\n",
    "\n",
    "from abe.metric.visualization import plot_explanation_heatmap\n",
    "\n",
    "plot_explanation_heatmap(attribution, sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
